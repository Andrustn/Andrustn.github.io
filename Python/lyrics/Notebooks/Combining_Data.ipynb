{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbcf5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import wordpunct_tokenize, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9924be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in each individual genre\n",
    "\n",
    "Rock = pd.read_csv('Rock.csv')\n",
    "Country = pd.read_csv('Country.csv').drop(columns = ['Unnamed: 0','Song'])\n",
    "Rap = pd.read_csv('Rap.csv').drop(columns = ['Unnamed: 0'])\n",
    "Pop = pd.read_csv('Pop.csv')\n",
    "RnB = pd.read_csv('RnB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9967052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine genres into one comprehensive dataframe, reset index\n",
    "\n",
    "Combined_Lyrics = Rock.append(Country, ignore_index = True).append(Rap, ignore_index = True).append(Pop, ignore_index = True).append(RnB, ignore_index = True).drop_duplicates().dropna()\n",
    "Combined_Lyrics = Combined_Lyrics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367507dd-0e25-486f-9d29-261b9b780464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of stop words, creating a lemmatizer\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "sw_special = [\"oh\",\"yeah\",\"got\",\"go\",\"get\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\",\"let\",\n",
    "             \"way\",\"cause\",\"like\",\"know\",\"back\",\"uh\",\"ooh\",\"urlcopyembedcopy\"]\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587aacbc-720b-41da-a72c-7b52555b59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to tokenize, lemmatize and clean data - saving tokens as lists\n",
    "# removing embedshare tags\n",
    "\n",
    "def clean_data(lyrics):\n",
    "    lyrics = lyrics.replace('\\u2005',' ').replace('\\n',' ').replace('\\u205f',\" \")\n",
    "    lyrics = re.sub(\"\\d\\dEmbedShare URLCopyEmbedCopy\", \"\", lyrics)\n",
    "    lyrics = re.sub(\"\\d\\*embedshare.*$\", \"\", lyrics)\n",
    "    lyrics = lyrics.lower() # coerce data to lower case\n",
    "    tokens = wordpunct_tokenize(lyrics) # tokenize individual words\n",
    "    tokens = [tok for tok in tokens if tok.isalnum()] # removing punctuation\n",
    "    tokens = [tok for tok in tokens if tok not in sw] # removing stop words\n",
    "    tokens = [tok for tok in tokens if tok not in sw_special] # removing special stop words found in lyrics\n",
    "    tokens = [wn.lemmatize(tok) for tok in tokens] # lematizing lyrics - reducing to base words\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd81d3d-792c-4286-b7d7-0d5acb79043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to dataframe\n",
    "\n",
    "Combined_Lyrics['Lyrics'] = Combined_Lyrics['Lyrics'].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6cfb322-3d48-42b0-ad50-844fc452a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to csv\n",
    "\n",
    "Combined_Lyrics.to_csv('Combined_Lyrics.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
